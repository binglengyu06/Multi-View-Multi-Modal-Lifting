The script implements a multimodal video classification pipeline using PyTorch, focusing on fusing thermal (Figure 1.1) and RGB (Figure 1.2) inputs across multiple camera viewpoints. The architecture supports two ViViT-based Transformer models: TriVivitModel (Figure 1.3) for three-view fusion and HexVivitModel (Figure 1.4) for six-view fusion. Each model processes spatiotemporal inputs encoded from sequential frames and aggregates information across modalities and viewpoints using self-attention mechanisms.

    Data loading is handled by a custom VideoDataset class, which dynamically loads frames from structured directories under class-specific and viewpoint-specific subfolders. The loader enforces a fixed number of frames per video sequence (num_frames=32), performing uniform sampling and zero-padding with black frames if needed. Images are resized, normalized, and converted to PyTorch tensors, with separate transformations defined for RGB and thermal modalities. For multimodal fusion (rgbthermal), the dataset returns a tuple of tensors (RGB, thermal).

    The training pipeline supports key configurations through command-line arguments, including model type, input modality, viewpoints, batch size, learning rate, and the number of epochs. The models are trained using stochastic gradient descent with cross-entropy loss. Mixed-precision training is enabled via torch.cuda.amp to improve performance and reduce memory usage. The train_model function includes a training-validation loop that tracks loss and accuracy per epoch, while evaluate_model calculates top-1 accuracy over the validation set using argmax on predicted logits.
